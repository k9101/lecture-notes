# Lecture 9 - January 22, 2018

## Backpropagration
Let's generalize gradient descent to work on a network with any number of layers.

Note the vector operations:
$$\frac{\delta E}{\delta w_{i,j}} = \frac{\delta E}{\delta y_i} \frac{\delta y_i}{\delta z_i} \frac{\delta z_i}{\delta w_{ij}}$$


