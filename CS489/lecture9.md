# Lecture 9 - January 22, 2018

## Backpropagration
Let's generalize gradient descent to work on a network with any number of layers.

Note the vector operations:
![latex-905b585a-aab3-4ff6-bf9a-ac65ba568892](data/lecture9/latex-905b585a-aab3-4ff6-bf9a-ac65ba568892.png)


